<!doctype html><html lang=en><head><title>Practical Papers: Chord, A Scalable Peer-To-Peer Lookup Protocol :: silly onions — Opinionated articles</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="The post is an explanation of the Chord protocol and discussion of my implementation of the same in Golang.  Chord is one of the original Distributed Hash Table projects from the MIT PDOS group at Computer Science and AI Laboratory, MIT. Here is a link to the original research for your reading pleasure. I was introduced to Chord by the book Distributed Systems by Maarten van Steen and Andrew S."><meta name=keywords content><meta name=robots content=noodp><link rel=canonical href=/posts/2020-28-01-chord/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/assets/red.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/img/favicon/red.png><meta name=twitter:card content=summary><meta name=twitter:title content="Practical Papers: Chord, A Scalable Peer-To-Peer Lookup Protocol :: silly onions — Opinionated articles"><meta name=twitter:description content="The post is an explanation of the Chord protocol and discussion of my implementation of the same in Golang.  Chord is one of the original Distributed Hash Table projects from the MIT PDOS group at Computer Science and AI Laboratory, MIT. Here is a link to the original research for your reading pleasure. I was introduced to Chord by the book Distributed Systems by Maarten van Steen and Andrew S."><meta name=twitter:site content=/><meta name=twitter:creator content><meta name=twitter:image content><meta property=og:locale content=en><meta property=og:type content=article><meta property=og:title content="Practical Papers: Chord, A Scalable Peer-To-Peer Lookup Protocol :: silly onions — Opinionated articles"><meta property=og:description content="The post is an explanation of the Chord protocol and discussion of my implementation of the same in Golang.  Chord is one of the original Distributed Hash Table projects from the MIT PDOS group at Computer Science and AI Laboratory, MIT. Here is a link to the original research for your reading pleasure. I was introduced to Chord by the book Distributed Systems by Maarten van Steen and Andrew S."><meta property=og:url content=/posts/2020-28-01-chord/><meta property=og:site_name content="Practical Papers: Chord, A Scalable Peer-To-Peer Lookup Protocol"><meta property=og:image content><meta property=og:image:width content=2048><meta property=og:image:height content=1024><meta property=article:section content=project><meta property=article:published_time content="2020-01-28 00:00:00 &#43;0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>sillyonions</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=/posts/2020-28-01-chord/>Practical Papers: Chord, A Scalable Peer-To-Peer Lookup Protocol</a></h1><div class=post-meta><span class=post-date>2020-01-28</span></div><span class=post-tags>#<a href=/tags/distributed-systems/>distributed-systems</a>&nbsp;
#<a href=/tags/golang/>golang</a>&nbsp;
#<a href=/tags/dht/>dht</a>&nbsp;
#<a href=/tags/chord/>chord</a>&nbsp;</span><div class=post-content><ul><li>The post is an explanation of the Chord protocol and discussion of my <a href=https://github.com/arush15june/chord-golang>implementation</a> of the same in Golang.</li></ul><p>Chord is one of the original Distributed Hash Table projects from the MIT PDOS group at Computer Science and AI Laboratory, MIT. <a href=https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf>Here</a> is a link to the original research for your reading pleasure. I was introduced to Chord by the book <a href=https://www.distributed-systems.net/>Distributed Systems by Maarten van Steen and Andrew S. Tanenbaum</a> (p. 246, p. 248) as a solution for implementing a decentralized naming system or a structured decentralized overlay network.</p><h2 id=whats-a-distributed-hash-table>Whats a Distributed Hash Table?</h2><ul><li>A Hash Table is a data structure which stores values associated with keys and allows direct access to the values using these associated keys. A Hash Table is called a Dictionary in Python and Map in Golang.</li></ul><p>A distributed hash table, a DHT is a hash table distributed across various nodes in the network. The system allows any node that is part of the network to lookup location (i.e IP Address, in most cases) of the node containing the value of the key (not the value of the key itself).</p><p>PS. A node is just a system running the DHT program that is part of the DHT network.</p><h2 id=how-chord-creates-a-dht>How Chord creates a DHT?</h2><p><a href=https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf>Chord</a> was introduced in 2001 as one of the four original DHT protocols (CAN, Tapestry, Pastry, and Chord). This post will only focus on Chord (mostly because I haven&rsquo;t read about the other protocols, yet).</p><p>Chord suggests creating a Ring shaped overlay network in which nodes are arranged based on a unique numeric ID computed on the node. This ID is generated by using a Hash function which computes a integer from the passed in bytestream, in this case the hostname of the node. Every node in the system has an integer value associated with it which can be compared with each other, thus the Chord ring can be created by arranging them in increasing order of value of this ID. This ID stay&rsquo;s consistent throughout lifetime of the system unless the network interface is changed.</p><p>Let there be 4 nodes: n1, n2, n3, n4 with ID&rsquo;s 100, 200, 300, 400 respectively and arrange them in a Ring overlay. n2 is the successor of n1, n3 is the successor of n2, and so on.</p><pre><code>    n1 ---------- n2 
  (100)          (200)
    |              |
    |              |
    |              |
    n4 ---------- n3
  (400)          (300)
</code></pre><p>The Hash function is the SHA1 Hash of a bytestream and from this 160bit hash a fixed amount of bits are extracted to create a numeric value, in our case it is 64bits to create a 64 bit unsigned integer.</p><pre><code class=language-go>    // SHA1 hash of the data and extract a 64 bit (8 byte) integer out of it.
    func Hash(data []byte) {
        ID := SHA1(data)[:8]
    }
</code></pre><p>A key in the DHT who&rsquo;s location is to be resolved is hashed using the same hashing function as described above. Let the Key be any bytestream, example: Key = &ldquo;UserData&rdquo;. The function Hash(Key) returns a numerical value which will lie between one of the nodes in the Chord ring.</p><pre><code>        Hash(Key) = 125
           ↓
    n1 ---------- n2 
  (100)          (200)
    |              |
    |              |
    |              |
    n4 ---------- n3
  (400)          (300)
</code></pre><p>The most important aim of the system is to implement the function <code>FindSuccessor(HashedKey)</code> to lookup the location of a key in the DHT. If we execute <code>FindSuccessor(Hash(Key))</code> on any of the nodes that are part of the network, It will return the node&rsquo;s location who&rsquo;s ID&rsquo;s numerical value lies just next to the value of the hashed key value. In our case <code>FindSuccessor(Hash(Key))</code> will return the location of node n2.</p><pre><code>    Hash(Key) FindSuccessor(Hash(Key))
        ↓         ↓
    n1 ---------- n2 
  (100)          (200)
    |              |
    |              |
    |              |
    n4 ---------- n3
  (400)          (300)
</code></pre><h2 id=functions-and-structures>Functions and Structures</h2><p>The paper provides pseudocode for the many functions required to implement the DHT, my implementation also follows the pseudocode as presented in the paper. The concept of a <strong>Finger table</strong> and <strong>Successor table</strong> are used to improve the efficiency of the protocol.</p><p>To increase the number of keys which converge after lookup on a single physical node, multiple <em>virtual nodes</em> can be deployed at once. Each virtual node can be contacted independently by other nodes in the network. Every node in the network stores a reference to its successor and predecessor and supports the <strong>FindSuccessor</strong> function to lookup key locations.</p><h3 id=findsuccessor>FindSuccessor</h3><p>The paper&rsquo;s pseudocode algorithm of <strong>FindSuccessor</strong> looks at the current node&rsquo;s successor&rsquo;s ID if it is the successor to the key to be looked up. If it is not, A finger table which stores references to different nodes in the network is used to find the node whose ID precedes the key, thus this node&rsquo;s successor is the successor of the key. This node is then asked to lookup the key.</p><pre><code class=language-c++>n.find_successor(id)
  if (id is between n and successor, inclusive of sucessor)
    return successor;
  else
    n0 = closest_preceding_node(id);
  return n0.find_successor(id);
</code></pre><p>A naive approach could be to keep asking the successor of the node to lookup the key, which will trigger it to ask it&rsquo;s own successor and so on. This is a rather inefficient solution, the finger table resolves this problem by creating a list of nodes in the network according to the formula for generating an ID: <strong>(n + 2<sup>i</sup>) mod 2<sup>m</sup></strong>, where m is the total nodes that can be in the finger table and i goes on till m.</p><p>The paper provides proof that using the finger table every lookup request, with high probability, can be resolved by contacting O(log N) nodes, where N is the number of nodes in the network.</p><pre><code class=language-c++>n.closest_preceding_node(id)
  for i = m downto 1
    if (finger[i] 2 (n; id))
    return finger[i];
  return n;
</code></pre><p>A node in the Chord ring contains reference to its successor, predecessor and a finger table which is list of different nodes in the network.</p><h3 id=fixing-fingers>Fixing Fingers</h3><p>The finger table is created and updated over the lifetime of the node. The paper suggests the method <strong>FixFingers</strong> to periodically update fingers in the finger table. This periodical update interval to update the finger table as suggested by the paper is 15 seconds.</p><p>On every execution of <strong>FixFingers</strong>, one entry in the finger table is updated.</p><pre><code class=language-c++>n.fix_fingers()
  next = next + 1;
  if (next &gt; m)
    next = 1;
  finger[next] = n.find_successor(n + 2^(next - 1) );
</code></pre><h3 id=creating-a-network>Creating a Network</h3><p>If a node wants to create a Chord ring and be the first node in the network. It can set its predecessor to be nil and keep the reference of the successor to be itself.</p><pre><code class=language-c++>n.create()
  predecessor = nil;
  successor = n;
</code></pre><h3 id=joining-a-network>Joining a Network</h3><p>If a node wants to join a Chord ring it needs to know the network location of atleast one other node in the network and contact it by asking this node to resolve the successor for itself.</p><pre><code class=language-c++>n.join(n1)
  predecessor = nil
  successor = n1.find_successor(n)
</code></pre><h3 id=dynamic-operations-and-failures>Dynamic Operations and Failures</h3><p>The network has to be dynamically updated to let nodes join the network at will and stale nodes be removed from the network and stabilize the ring again. The paper suggests a stabilization routine to regularly verify that the current successor is still its successor and no new node has joined in between them in the ring.</p><p>The <strong>Stabilize</strong> routine takes the ID of the predecessor node of its successor (Each node stores the reference of its predecessor and successor) and verifies it to be itself. If it is not, it is verified if this new node lies in between itself and successor which makes it to be the successor of the node rather than the previous believed successor. This new node is also notified of its new predecessor. Thus, stabilizing this 3 node arc of the Ring with a newly joined node and informing all the nodes of its existence.</p><pre><code class=language-c++>n.stabilize()
  x = successor.predecessor
  if(x is in between n and successor)
    successor = x
  successor.notify(n)
</code></pre><pre><code class=language-c++>n.notify(n1)
  if(predecessor is nil or n1 is in between predecessor and n)
    predecessor = n1
</code></pre><p>Another function, <strong>CheckPredecessor</strong> also runs periodically to verify the liveness of the predecessor. The time interval for CheckPredecessor is suggested to be 15 seconds by the paper.</p><pre><code class=language-c++>n.check_predecessor()
  if(predecessor has failed)
    predecessor = nil
</code></pre><h2 id=implementation-in-golang>Implementation in Golang</h2><p><strong><a href=https://github.com/arush15june/chord-golang>Source Code on GitHub</a></strong></p><p>As explained above, a Chord Ring (or, network) consists of nodes constantly updating their finger table, stabilizing their position in the ring to let new nodes join, verifying the liveness of their predecessor, and looking up the location of keys using the finger tables. Thus creating a dynamically updating and decentralized network for looking up keys i.e a Distributed Hash Table.</p><p>I have implemented Chord in Golang, recreating the pseudocode in the paper. The concepts of a virtual node is implemented in the form of indepedent threads running on different ports on the same system, this enables every local node to be access via the network. The nodes are created in a manner which allows Local Nodes to be directly contacted by Direct Method calls rather than unneccesarily sending RPC&rsquo;s over the network. RPC is implemented using Golang&rsquo;s internal gob encoding based RPC mechanism.</p><p>To enable the intermingling of local and remote nodes, a common interface VNodeProtocol is defined which is implemented by two different type of VNodes: LocalVNode and RemoteVNode. While the LocalVNode contains the logic for the Chord protocol, the RemoteVNode are just RPC Wrappers to contact LocalVNodes running on remote systems. VNodeProtocol allows the protocol to transparently handle both Remotely and Locally available nodes. Locally running worker threads contact each other using Direct Method calls, and calling the same methods via RPC for RemoteVNodes.</p><pre><code class=language-text>      ---&gt; LocalVNode: Local Implementation of a VNode.
      |                Contains implementation of the Chord Protocol
      |
VNode.VNodeProtcol ---&gt; Generic Interface to the Chord Protocol.
      |
      |
      ---&gt; RemoteVNode: RPC Backed VNode which communicates to the RPC Server
                        (defined in `rpcserver.go`) which calls the remote
                        processes' LocalVNode to fulfil the RPC.
</code></pre><p>The finger and successor tables are lists of VNodeProtocol objects, A hostname is wrapped as a RemoteVNode object to join a Chord ring, Each individual VNode on the machine is a LocalVNode in its own goroutine running the stabilization, finger fixing, and predecessor verification routines alongwith running a RPC server in the background.</p><pre><code class=language-go>// VNodeProtocol implements the Chord protocol on Vnodes.
// Local VNodes can implement it via method calls.
// Remote VNodes can use RPC to transparently work like Local VNodes.
type VNodeProtocol interface {
	// FindSuccessors finds N successors of the VNode.
	FindSuccessors(int) ([]VNodeProtocol, error)

	// FindSuccessor finds the successor for a Key.
	FindSuccessor(uint64) (VNodeProtocol, error)

	// Notify notifies the VNode of its new predecessor.
	Notify(VNodeProtocol) error

	// Ping sends a request to a VNode
	Ping() error

	// CheckPredecessor checks the aliveness of VNode's predecessor.
	CheckPredecessor() error

	// GetPredecessor returns the predecessor VNode.
	GetPredecessor() (VNodeProtocol, error)

	// IsBetweenNodes
	IsBetweenNodes(VNodeProtocol, VNodeProtocol) bool

	// ID returns the ID of the VNode.
	ID() uint64

	// Hostname returns the hostname of the VNode.
	Hostname() string
}
</code></pre><h3 id=implementing-periodical-routines>Implementing periodical routines.</h3><p>The periodically running routines are each initialized as their own goroutine when the LocalVNode is first initialized along with their respective time intervals. An example of the Stabilization routine is given below. Where the Stabilize function is execeuted in a time interval between <code>[node.minStabilizeInterval, node.maxStabilizeInterval]</code>.</p><pre><code class=language-go>// Stabilize executes after certain time intervals to fix successors.
// &gt; Each time node n runs Stabilize(), it asks its successor
// &gt; for the successor’s predecessor p, and decides whether p
// &gt; should be n’s successor instead. stabilize() notifies node
// &gt; n’s successor of n’s existence, giving the successor the chance
// &gt; to change its predecessor to n.
func (node *LocalVNode) Stabilize() error {
	logger.Printf(&quot;[%s, %d] Stabilizing VNode\n&quot;, node.Hostname(), node.ID())

	verifySuccesorNode, _ := node.successors[0].GetPredecessor()
	if verifySuccesorNode != nil &amp;&amp; verifySuccesorNode.IsBetweenNodes(node, node.successors[0]) {
		node.successors[0] = verifySuccesorNode
		logger.Printf(&quot;[%s, %d] Updated successor: %s\n&quot;, node.Hostname(), node.ID(), verifySuccesorNode.Hostname())
	}

	if node.successors[0].ID() != node.ID() {
		err := node.successors[0].Notify(node)
		logger.Printf(&quot;[%s, %d] Notified %s of VNode.\n&quot;, node.Hostname(), node.ID(), node.successors[0].Hostname())
		return err
	}

	return nil
}

// StabilizeRoutine runs Stabilize() periodically by choosing an interval
// between minStabilizeInterval and maxStabilizeInterval.
func (node *LocalVNode) StabilizeRoutine() error {
	exit := false

	for {
		node.Stabilize()

		interval := Util.GetRandomBetween(node.minStabilizeInterval, node.maxStabilizeInterval)
		timer := time.NewTimer(time.Duration(interval) * time.Second)
		select {
		case &lt;-timer.C:
		case &lt;-node.stopStabilizeChan:
			exit = true
		}
		if exit {
			break
		}
	}
	return nil
}

// GetRandomBetween returns a random integer value between low and high.
func GetRandomBetween(low int, high int) int {
	return rand.Intn(high-low) + low
}

</code></pre><h3 id=hostnames-and-rpc>Hostnames and RPC</h3><p>RPC is implemented using the net/rpc package which internally serializes structures and transmits them over the network. A TCP RPC Server is kept running by each LocalVNode for remote nodes to contact them.</p><p>The RPC Server can take in the hostnames for running the server in various forms. It depends on the input to net.Listen (as can be seen in the implementation of InitServer).</p><p>The user can pass in a hostname like &ldquo;192.168.1.116:8000&rdquo; which will bind port 8000 on the interface with the IP 192.168.1.116 on the physical system. If the user wants a random port to be allocated by the kernel itself, they can supply a hostname address like &ldquo;192.168.1.116:&rdquo; binding a random port as supplied by the kernel for the RPC server. This is reflected in the Hostname field of the LocalVNode and is transmitted to RemoteVNode in RPC arguments and results. Thus it is important to consider the hostname format. A hostname like &ldquo;[::]:0&rdquo; will bind a random port on 0.0.0.0 but will break the system when passed onto a remote VNode via RPC.</p><pre><code class=language-go>// InitServer starts Chord Protocol TCP-RPC server on Hostname:Port.
func InitServer(rpcInstance *ChordTCPRPCServer) error {
	rpc.Register(rpcInstance)
	l, e := net.Listen(&quot;tcp&quot;, rpcInstance.Hostname)
	if e != nil {
		return errors.New(&quot;failed to start Listen server&quot;)
	}

	// Reset address as acquired by Listener.
	address := l.Addr().String()
	rpcInstance.Hostname = address

	go func() {
		for {
			conn, err := l.Accept()
			if err != nil {
				continue
			}
			go rpc.ServeConn(conn)
		}
	}()

	return nil
}
</code></pre><h3 id=http-server-for-key-lookup>HTTP Server For Key Lookup</h3><p>A Single HTTP Server is hosted which uses one of the globally accessible worker nodes to lookup a key passed in as form data in a HTTP POST request.</p><p>The <code>/lookup</code> endpoint returns the location of the node where the key will be stored.</p><pre><code class=language-bash>$ curl -X POST localhost:8090/lookup -d &quot;TestKey&quot;
127.0.0.1:8000
</code></pre><h3 id=some-examples>Some examples</h3><p>The project doesn&rsquo;t have any external dependencies. I developed the project using Go 1.12 on Windows 10 1909.</p><ul><li><p>Build the program.</p><pre><code>go build
</code></pre></li><li><p>Run a single worker thread on <strong>127.0.0.1:8000</strong>.</p><pre><code>./src -host 127.0.0.1:8000
</code></pre></li><li><p>Connect to a remote host.</p><pre><code>./src -mode join -httpport 8091 -host 127.0.0.1:8001 -rhost 127.0.0.1:8000
</code></pre></li><li><p>Run 8 Local Worker Threads on Randomly Assigned Ports <strong>(0.0.0.0:0)</strong>.</p><pre><code>./src -workers 8
</code></pre></li></ul><h2 id=conclusion>Conclusion</h2><p>The Chord protocol is implemented, however there is still polishing to be done for improving, configuration of the workers (possibily a YAML file to define them?). This is one of the firsts for me where I went from reading a paper to implementing it and many open source implementations helped me throughout this process: <a href=https://github.com/armon/go-chord>armon/go-chord</a>, <a href=https://github.com/cbocovic/chord>cbocovic/chord</a> and <a href=https://github.com/yuma-m/pychord>yuma-m/pychord</a>.</p><h2 id=references>References</h2><ul><li><a href=https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf>Original Research Paper for Chord</a></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=/posts/2019-24-11-hacking-qatar/><span class=button__text>Bits &amp; Signals: Qatar International Cybersecurity Competition</span>
<span class=button__icon>→</span></a></span></div></div><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"arush15june-github-io"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2020 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/assets/main.js></script><script src=/assets/prism.js></script></div></body></html>